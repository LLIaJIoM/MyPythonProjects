import feedparser
import requests
from datetime import datetime, timedelta
import time

def get_latest_news(rss_url, limit=5):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –ª–µ–Ω—Ç—ã"""
    try:
        # –ü–∞—Ä—Å–∏–º RSS –ª–µ–Ω—Ç—É
        feed = feedparser.parse(rss_url)
        
        if feed.bozo:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS: {feed.bozo_exception}")
            return []
        
        news_list = []
        
        for entry in feed.entries[:limit]:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            title = entry.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
            summary = entry.get('summary', '')
            link = entry.get('link', '')
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç HTML —Ç–µ–≥–æ–≤
            summary = clean_html(summary)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç—å –Ω–µ —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è (–Ω–µ —Å—Ç–∞—Ä—à–µ 24 —á–∞—Å–æ–≤)
            if 'published_parsed' in entry:
                pub_date = datetime(*entry.published_parsed[:6])
                if datetime.now() - pub_date > timedelta(hours=24):
                    continue
            
            news_list.append({
                'title': title,
                'summary': summary,
                'link': link,
                'published': entry.get('published', '')
            })
        
        print(f"üì∞ –ü–æ–ª—É—á–µ–Ω–æ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {rss_url}")
        return news_list
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        return []

def clean_html(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç HTML —Ç–µ–≥–æ–≤"""
    import re
    # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
    clean = re.compile('<.*?>')
    text = re.sub(clean, '', text)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = ' '.join(text.split())
    return text

def get_news_from_lenta():
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Lenta.ru"""
    return get_latest_news("https://lenta.ru/rss/news", limit=3)

def get_news_from_ria():
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏"""
    return get_latest_news("https://ria.ru/export/rss2/archive/index.xml", limit=3)

def get_news_from_tass():
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –¢–ê–°–°"""
    return get_latest_news("https://tass.ru/rss/v2.xml", limit=3)

def get_combined_news():
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ —Å –õ–µ–Ω—Ç–∞.—Ä—É"""
    all_news = []
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ —Å –õ–µ–Ω—Ç–∞.—Ä—É
    try:
        news = get_news_from_lenta()
        for item in news:
            item['source'] = "Lenta.ru"
        all_news.extend(news)
        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π —Å –õ–µ–Ω—Ç–∞.—Ä—É")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —Å –õ–µ–Ω—Ç–∞.—Ä—É: {e}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
    all_news.sort(key=lambda x: x.get('published', ''), reverse=True)
    
    return all_news[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-5 –Ω–æ–≤–æ—Å—Ç–µ–π

if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources = [
        ("Lenta.ru", "https://lenta.ru/rss/news"),
        ("–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏", "https://ria.ru/export/rss2/archive/index.xml"),
        ("–¢–ê–°–°", "https://tass.ru/rss/v2.xml")
    ]
    
    for source_name, url in sources:
        print(f"\nüì∞ –¢–µ—Å—Ç–∏—Ä—É–µ–º {source_name}:")
        news = get_latest_news(url, limit=2)
        for i, item in enumerate(news, 1):
            print(f"  {i}. {item['title'][:50]}...")
            print(f"     {item['summary'][:100]}...") 