from flask import Flask, request, jsonify
from flask_cors import CORS
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ G4F
sys.path.append('/app/g4f')

from g4f import ChatCompletion, Provider
import asyncio
import json

app = Flask(__name__)
CORS(app)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∞
last_response = ""

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    global last_response
    
    try:
        data = request.get_json()
        messages = data.get('messages', [])
        model = data.get('model', 'gpt-3.5-turbo')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
        prompt = ""
        for message in messages:
            role = message.get('role', 'user')
            content = message.get('content', '')
            if role == 'user':
                prompt += f"User: {content}\n"
            elif role == 'assistant':
                prompt += f"Assistant: {content}\n"
            elif role == 'system':
                prompt += f"System: {content}\n"
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
        prompt = prompt.strip()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ G4F
        response = asyncio.run(generate_response(prompt))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç
        last_response = response
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
        return jsonify({
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(response.split()),
                "total_tokens": len(prompt.split()) + len(response.split())
            }
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

async def generate_response(prompt):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ G4F"""
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
        providers = [
            Provider.DeepInfra,
            Provider.DeepSeek,
            Provider.Gemini,
            Provider.GeminiPro
        ]
        
        for provider in providers:
            try:
                response = await ChatCompletion.create_async(
                    model="gpt-3.5-turbo",
                    provider=provider,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º {provider}: {e}")
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "healthy", "last_response": last_response})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8888))
    print(f"üöÄ –ó–∞–ø—É—Å–∫ G4F —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É {port}...")
    app.run(host='0.0.0.0', port=port, debug=False) 